# Dictionnaire des LLMs quantifies pour une generation local
MODEL_INFOS = {
    "Mistral 7B - Q4_K_M": {
        "filename": "mistral-7b-instruct-v0.1.Q4_K_M.gguf",
        "url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf",
        "chemin": "backend/app/rag_multiagents/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf"

        
    },

    "Meta-Llama-3.1-8B-Instruct-IQ4_XS": {
        "filename": "Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf",
        "url": " https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf?download=true",
        "chemin": "backend/app/rag_multiagents/models/Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf"
    },

}
